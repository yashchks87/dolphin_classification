

Epoch: 0:   0%|          | 0/32 [00:01<?, ?batch/s]
tensor(3.3767, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.2492, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.4209, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.4154, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.3401, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.2178, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.1665, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.3960, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.3518, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.1449, device='cuda:0', grad_fn=<DivBackward1>)

Epoch: 0:  34%|███▍      | 11/32 [00:07<00:03,  5.73batch/s, loss=3.27]
tensor(3.2173, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.3133, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.2385, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.0678, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.1927, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.0468, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.1639, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.0022, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.0195, device='cuda:0', grad_fn=<DivBackward1>)
tensor(2.9092, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.0701, device='cuda:0', grad_fn=<DivBackward1>)
tensor(2.9381, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.1539, device='cuda:0', grad_fn=<DivBackward1>)
tensor(2.9519, device='cuda:0', grad_fn=<DivBackward1>)
tensor(2.9191, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.0368, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.0554, device='cuda:0', grad_fn=<DivBackward1>)

Epoch: 0:  91%|█████████ | 29/32 [00:09<00:00,  8.47batch/s, loss=3.08]
tensor(2.9832, device='cuda:0', grad_fn=<DivBackward1>)
tensor(3.0190, device='cuda:0', grad_fn=<DivBackward1>)
tensor(2.9866, device='cuda:0', grad_fn=<DivBackward1>)
Epoch: 0: 100%|██████████| 32/32 [00:10<00:00,  3.07batch/s, loss=2.99]


Epoch: 0:   0%|          | 0/3126 [00:04<?, ?batch/s]
torch.Size([16, 30])
torch.Size([16, 30])
Using cache found in /home/ubuntu/.cache/torch/hub/pytorch_vision_v0.10.0
/usr/lib/python3/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/lib/python3/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
/usr/lib/python3/dist-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.
  warnings.warn(
Using cache found in /home/ubuntu/.cache/torch/hub/pytorch_vision_v0.10.0
/usr/lib/python3/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/lib/python3/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
/usr/lib/python3/dist-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.
  warnings.warn(