




























































































































Epoch: 0: 100%|██████████| 275/275 [04:24<00:00,  1.04batch/s, loss=2.49]
train Loss: 0.01554332997695193
Epoch: 0: 100%|██████████| 3/3 [00:13<00:00,  4.56s/batch, loss=2.64]
  0%|          | 0/275 [00:00<?, ?batch/s]
val Loss: 0.015694588069388046
























































































































Epoch: 1: 100%|█████████▉| 274/275 [04:12<00:00,  1.20batch/s, loss=2.68]
Epoch: 1: 100%|██████████| 275/275 [04:13<00:00,  1.09batch/s, loss=2.65]

Epoch: 1:  33%|███▎      | 1/3 [00:12<00:24, 12.45s/batch, loss=2.49]
val Loss: 0.015195534634495913
Epoch: 1: 100%|██████████| 3/3 [00:13<00:00,  4.45s/batch, loss=2.69]


























































































































Epoch: 2: 100%|██████████| 275/275 [04:17<00:00,  1.19batch/s, loss=2.51]
Epoch: 2: 100%|██████████| 275/275 [04:18<00:00,  1.07batch/s, loss=2.51]

Epoch: 2: 100%|██████████| 3/3 [00:13<00:00,  4.49s/batch, loss=2.58]
val Loss: 0.014856148614242615
Weight saved for epoch 2.



























































































































Epoch: 3: 100%|██████████| 275/275 [04:16<00:00,  1.07batch/s, loss=2.51]
train Loss: 0.013566438549615905
Epoch: 3: 100%|██████████| 3/3 [00:13<00:00,  4.51s/batch, loss=2.56]
  0%|          | 0/275 [00:00<?, ?batch/s]
val Loss: 0.014681104614800615
























































































































Epoch: 4: 100%|█████████▉| 274/275 [04:11<00:00,  1.21batch/s, loss=2.32]
Epoch: 4: 100%|██████████| 275/275 [04:13<00:00,  1.09batch/s, loss=2.29]
Epoch: 4: 100%|██████████| 3/3 [00:12<00:00,  4.25s/batch, loss=2.47]
  0%|          | 0/275 [00:00<?, ?batch/s]
val Loss: 0.014205870892219393



























































































































Epoch: 5: 100%|██████████| 275/275 [04:18<00:00,  1.19batch/s, loss=2.57]
Epoch: 5: 100%|██████████| 275/275 [04:18<00:00,  1.06batch/s, loss=2.57]

Epoch: 5:  67%|██████▋   | 2/3 [00:13<00:05,  5.48s/batch, loss=2.32]
val Loss: 0.013931756905416254
Epoch: 5: 100%|██████████| 3/3 [00:13<00:00,  4.56s/batch, loss=2.31]

























































































































Epoch: 6: 100%|█████████▉| 274/275 [04:15<00:00,  1.15batch/s, loss=2.31]
Epoch: 6: 100%|██████████| 275/275 [04:16<00:00,  1.07batch/s, loss=2.26]

Epoch: 6: 100%|██████████| 3/3 [00:13<00:00,  3.05s/batch, loss=2.33]
val Loss: 0.013769085699390516
Epoch: 6: 100%|██████████| 3/3 [00:13<00:00,  4.46s/batch, loss=2.33]

























































































































Epoch: 7: 100%|██████████| 275/275 [04:15<00:00,  1.23batch/s, loss=2.18]
Epoch: 7: 100%|██████████| 275/275 [04:15<00:00,  1.07batch/s, loss=2.18]

Epoch: 7: 100%|██████████| 3/3 [00:13<00:00,  4.50s/batch, loss=2.24]
val Loss: 0.013776518139443378
Weight saved for epoch 7.























































































































Epoch: 8: 100%|██████████| 275/275 [04:12<00:00,  1.09batch/s, loss=2.24]
  0%|          | 0/3 [00:00<?, ?batch/s]
Epoch: 8: 100%|██████████| 3/3 [00:13<00:00,  4.48s/batch, loss=2.35]
  0%|          | 0/275 [00:00<?, ?batch/s]
val Loss: 0.013371474186893509


























































































































Epoch: 9: 100%|██████████| 275/275 [04:18<00:00,  1.06batch/s, loss=2.22]
  0%|          | 0/3 [00:00<?, ?batch/s]

Epoch: 9: 100%|██████████| 3/3 [00:13<00:00,  4.55s/batch, loss=2.47]
val Loss: 0.013171236034438543
Weight saved for epoch 9.
Using cache found in /home/ubuntu/.cache/torch/hub/pytorch_vision_v0.10.0
/usr/lib/python3/dist-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.
  warnings.warn(